{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0618b3f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab02299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "from time import strftime\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(888)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(404)\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a603138b",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e00008",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_logs/'\n",
    "NR_CLASSES = 10\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "IMG_PIXELS = IMG_HEIGHT*IMG_WIDTH\n",
    "COLOR_CHANNELS = 1\n",
    "TOTAL_INPUTS = IMG_PIXELS*COLOR_CHANNELS\n",
    "\n",
    "VALIDATION_SIZE =  10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe34129",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac00291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e19f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e17498b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44800c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aeb6b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.27 s\n",
      "Wall time: 6.73 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)\n",
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cda542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 750 ms\n",
      "Wall time: 1.14 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65733b",
   "metadata": {},
   "source": [
    "# Explore and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a300f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-Scale\n",
    "x_train_all, x_test = x_train_all/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630ffd6",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edf884a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]\n",
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b02c1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580473a",
   "metadata": {},
   "source": [
    "#### Create Validation Dataset from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519a0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac8a8d",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "423386d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NR_CLASSES], name='Labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea4bcd",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d6a16",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea40898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7334f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('hidden_layer_1'):\n",
    "#     initial_w1 = tf.truncated_normal(shape=[TOTAL_INPUTS,n_hidden1], stddev=0.1, seed=42)\n",
    "#     w1 = tf.Variable(initial_value=initial_w1,name= 'w1')\n",
    "#     initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\n",
    "#     b1 = tf.Variable(initial_value=initial_b1,name= 'b2')\n",
    "#     layer1_in = tf.matmul(X, w1) + b1\n",
    "#     layer1_out = tf.nn.relu(layer1_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8bfc0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('hidden_layer_2'):\n",
    "#     initial_w2 = tf.truncated_normal(shape=[n_hidden1,n_hidden2], stddev=0.1, seed=42)\n",
    "#     w2 = tf.Variable(initial_value=initial_w2,name= 'w2')\n",
    "\n",
    "#     initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\n",
    "#     b2 = tf.Variable(initial_value=initial_b2,name= 'b2')\n",
    "\n",
    "#     layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "#     layer2_out = tf.nn.relu(layer2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a99aaa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('output_layer'):\n",
    "    \n",
    "#     initial_w3 = tf.truncated_normal(shape=[n_hidden2,NR_CLASSES], stddev=0.1, seed=42)\n",
    "#     w3 = tf.Variable(initial_value=initial_w3,name= 'w3')\n",
    "\n",
    "#     initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\n",
    "#     b3 = tf.Variable(initial_value=initial_b3,name= 'b3')\n",
    "\n",
    "#     layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "#     output = tf.nn.softmax(layer3_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0b786367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\n",
    "    with tf.name_scope(name):\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "        w = tf.Variable(initial_value=initial_w,name= 'W')\n",
    "\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "        b = tf.Variable(initial_value=initial_b,name= 'B')\n",
    "\n",
    "        layer_in = tf.matmul(input, w) + b\n",
    "        \n",
    "        if name=='out':\n",
    "            layer_out = tf.nn.softmax(layer_in)\n",
    "        else:\n",
    "            layer_out = tf.nn.relu(layer_in)\n",
    "            \n",
    "        tf.summary.histogram('weights', w)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "694e9110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model without dropout\n",
    "\n",
    "# layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1],\n",
    "#                       bias_dim=[n_hidden1], name='layer_1')\n",
    "# layer_2 = setup_layer(layer_1, weight_dim=[n_hidden1, n_hidden2],\n",
    "#                       bias_dim=[n_hidden2], name='layer_2')\n",
    "# output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES],\n",
    "#                       bias_dim=[NR_CLASSES], name='out')\n",
    "# model_name = f'{n_hidden1}-{n_hidden2} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d6081c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1],\n",
    "                      bias_dim=[n_hidden1], name='layer_1')\n",
    "layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2],\n",
    "                      bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES],\n",
    "                      bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2784ae5",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5500a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directories\n"
     ]
    }
   ],
   "source": [
    "folder_name = f'{model_name} at {strftime(\"%H %M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    print(exception.strerror)\n",
    "else:\n",
    "    print('Successfully created directories')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c490bfb",
   "metadata": {},
   "source": [
    "# Loss, Optimisation & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eae474",
   "metadata": {},
   "source": [
    "#### Defining Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "750435d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss_calc'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033f016",
   "metadata": {},
   "source": [
    "#### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8552e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a9cb2",
   "metadata": {},
   "source": [
    "#### Accuracy Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "12515ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\n",
    "    model_prediction = tf.argmax(output, axis=1, name='prediction')\n",
    "    correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "17164f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\n",
    "    tf.summary.scalar('acccuracy', accuracy)\n",
    "    tf.summary.scalar('cost', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b634d",
   "metadata": {},
   "source": [
    "#### Check Input Images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "29eb6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\n",
    "    x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "    tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a3429",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ea772276",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f4b9e",
   "metadata": {},
   "source": [
    "#### Setup Filewriter and Merge Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b9adc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(directory+'/train')\n",
    "train_writer.add_graph(sess.graph)\n",
    "\n",
    "validation_writer = tf.summary.FileWriter(directory+'/validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b89325",
   "metadata": {},
   "source": [
    "#### Initialise all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f13abfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc8660",
   "metadata": {},
   "source": [
    "## Batching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "724dc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2751c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples/size_of_batch)\n",
    "\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2421bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    \n",
    "    global num_examples\n",
    "    global index_in_epoch\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch = index_in_epoch + batch_size\n",
    "    \n",
    "    if index_in_epoch>num_examples:\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    \n",
    "    end = index_in_epoch\n",
    "    \n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e4baddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.8379999995231628\n",
      "Epoch 1 \t| Training Accuracy = 0.8550000190734863\n",
      "Epoch 2 \t| Training Accuracy = 0.8659999966621399\n",
      "Epoch 3 \t| Training Accuracy = 0.8669999837875366\n",
      "Epoch 4 \t| Training Accuracy = 0.8759999871253967\n",
      "Epoch 5 \t| Training Accuracy = 0.9729999899864197\n",
      "Epoch 6 \t| Training Accuracy = 0.9760000109672546\n",
      "Epoch 7 \t| Training Accuracy = 0.9800000190734863\n",
      "Epoch 8 \t| Training Accuracy = 0.9829999804496765\n",
      "Epoch 9 \t| Training Accuracy = 0.9850000143051147\n",
      "Epoch 10 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 11 \t| Training Accuracy = 0.9850000143051147\n",
      "Epoch 12 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 13 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 14 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 15 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 16 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 17 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 18 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 19 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 20 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 21 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 22 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 23 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 24 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 25 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 26 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 27 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 28 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 29 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 30 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 31 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 32 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 33 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 34 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 35 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 36 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 37 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 38 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 39 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 40 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 41 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 42 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 43 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 44 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 45 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 46 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 47 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 48 \t| Training Accuracy = 0.9940000176429749\n",
      "Epoch 49 \t| Training Accuracy = 0.9940000176429749\n",
      "Done training!!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nr_epochs):\n",
    "    #=============================Train===========================\n",
    "    for i in range(nr_iterations):\n",
    "        \n",
    "        batch_x, batch_y = next_batch(size_of_batch, x_train, y_train)\n",
    "        \n",
    "        feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        \n",
    "        sess.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    s,batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "    \n",
    "    train_writer.add_summary(s, epoch)    \n",
    "    \n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "    #=============================Validation===========================\n",
    "    \n",
    "    summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
    "    validation_writer.add_summary(summary, epoch)\n",
    "print('Done training!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34be698",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35763c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kiran Bharadwaj HD\\AppData\\Local\\Temp\\ipykernel_15756\\1641147386.py:3: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:203: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: SavedModel\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "outputs = {'accuracy_calc/prediction': model_prediction}\n",
    "inputs = {'X':X}\n",
    "tf.compat.v1.saved_model.simple_save(sess, 'SavedModel', inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385af566",
   "metadata": {},
   "source": [
    "## Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8be7835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('MNIST/test_img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eafb8fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nMVSwRGEIAxcHP+WcLZgBch15UtTynVgBcylE0vwKsg9HAhw6sNx5vYTwpIlGzCCY1Qn3G0kUZYabYjAAPxuJTGs90hr67D4vOcGgOVUVwKWLbgp7ojKPrZgr1kpITnWftWkzo8Sd82OFQDAE3Y8kB36Ib9Dycm5pWggyBIXikAcPL1adT8WZDpv1peRX8Tpmr9/sNvIL4ZNnNSjtr9VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw = img.convert('L')\n",
    "bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3d092bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array = np.invert(bw)\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7d190ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = img_array.ravel()\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5019e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sess.run(fetches=tf.argmax(output, axis=1), \n",
    "                      feed_dict={X:[test_img]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4ee60e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is [2]\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642c943",
   "metadata": {},
   "source": [
    "## Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f80f22c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mX\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m:\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy for test data set is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1116\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# Check session.\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m-> 1116\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempted to use a closed Session.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1118\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe Session graph is empty. Add operations to the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1119\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph before calling run().\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "test_accuracy = sess.run(fetches=accuracy, feed_dict={X:x_test, Y:y_test}) \n",
    "print(f'Accuracy for test data set is {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5951e5",
   "metadata": {},
   "source": [
    "## Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e1bbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer.close()\n",
    "validation_writer.close()\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a091db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
